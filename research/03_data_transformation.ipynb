{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\End_to_end_Employee\\\\End_to_End_Implementation\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Jaison\\\\Documents\\\\Workspace\\\\Main Projects\\\\End_to_end_Employee\\\\End_to_End_Implementation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Employee_Attition_End_to_end_ML_project_with_MLflow.constants import *\n",
    "from Employee_Attition_End_to_end_ML_project_with_MLflow.utils.common import read_yaml, create_directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Employee_Attition_End_to_end_ML_project_with_MLflow import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def data_preprocessing(self):\n",
    "        df = pd.read_csv(self.config.data_path)\n",
    "        # Interaction Features\n",
    "        df['TWY_JobLevel'] = df['TotalWorkingYears'] * df['JobLevel']\n",
    "        # Aggregation Features\n",
    "        df['Income_to_Age'] = df['MonthlyIncome'] / df['Age']\n",
    "        # Polynomial Features (as an example, we'll square the YearsAtCompany)\n",
    "        df['YearsAtCompany_sq'] = df['YearsAtCompany']**2\n",
    "        df[['TWY_JobLevel', 'Income_to_Age', 'YearsAtCompany_sq']].head()\n",
    "        # Drop features that provide no information\n",
    "        df.drop(columns=['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], inplace=True)\n",
    "        # Define numerical and categorical features\n",
    "        numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        # Remove target variable from categorical features\n",
    "        categorical_features.remove('Attrition')\n",
    "\n",
    "        # Define the preprocessing pipeline for numerical features\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        # Define the preprocessing pipeline for categorical features\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('onehot', OneHotEncoder(drop='first'))\n",
    "        ])\n",
    "\n",
    "        # Combine the preprocessing pipelines\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, numerical_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "\n",
    "        # Apply the preprocessing pipeline to the data\n",
    "        X = preprocessor.fit_transform(df)\n",
    "        y = df['Attrition'].map({'Yes': 1, 'No': 0}).values\n",
    "\n",
    "        # FEATURE SELECTION\n",
    "\n",
    "        # Apply ANOVA F-statistic for numerical features\n",
    "        anova_selector = SelectKBest(score_func=f_classif, k='all')\n",
    "        X_numerical = preprocessor.transformers_[0][1].fit_transform(df[numerical_features])\n",
    "        anova_selector.fit(X_numerical, y)\n",
    "\n",
    "        # Apply chi-squared test for categorical features\n",
    "        chi2_selector = SelectKBest(score_func=chi2, k='all')\n",
    "        X_categorical = preprocessor.transformers_[1][1].fit_transform(df[categorical_features])\n",
    "        chi2_selector.fit(X_categorical, y)\n",
    "\n",
    "        # Calculate feature importance scores using Random Forest\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        rf.fit(X, y)\n",
    "        feature_importances = rf.feature_importances_\n",
    "\n",
    "        # Perform Recursive Feature Elimination (RFE)\n",
    "        rfe = RFE(estimator=RandomForestClassifier(random_state=42), n_features_to_select=15)\n",
    "        rfe.fit(X, y)\n",
    "\n",
    "        # Get the feature names after one-hot encoding\n",
    "        encoded_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "\n",
    "        # Combine the numerical and encoded categorical feature names\n",
    "        all_features = numerical_features + encoded_features.tolist()\n",
    "\n",
    "        # Update the feature scores DataFrame\n",
    "        feature_scores = pd.DataFrame({\n",
    "            'Features': all_features,\n",
    "            'ANOVA F-statistic': list(anova_selector.scores_) + ['N/A'] * len(encoded_features),\n",
    "            'Chi-Squared Test': ['N/A'] * len(numerical_features) + list(chi2_selector.scores_),\n",
    "            'Feature Importances': feature_importances,\n",
    "            'RFE Support': rfe.support_,\n",
    "            'RFE Ranking': rfe.ranking_\n",
    "        })\n",
    "        # ANOVA F-statistic for numerical features\n",
    "        anova_scores = pd.DataFrame({\n",
    "            'Numerical Features': numerical_features,\n",
    "            'ANOVA F-statistic': anova_selector.scores_\n",
    "        })\n",
    "        anova_scores = anova_scores.sort_values(by='ANOVA F-statistic', ascending=False)\n",
    "\n",
    "        # Chi-Squared Test for categorical features\n",
    "        chi2_scores = pd.DataFrame({\n",
    "            'Categorical Features': encoded_features,\n",
    "            'Chi-Squared Test': chi2_selector.scores_\n",
    "        })\n",
    "        chi2_scores = chi2_scores.sort_values(by='Chi-Squared Test', ascending=False)\n",
    "\n",
    "        # Feature Importance Scores from Random Forest\n",
    "        feature_importance_scores = pd.DataFrame({\n",
    "            'Features': all_features,\n",
    "            'Feature Importances': feature_importances\n",
    "        })\n",
    "        feature_importance_scores = feature_importance_scores.sort_values(by='Feature Importances', ascending=False)\n",
    "\n",
    "        # Recursive Feature Elimination (RFE)\n",
    "        rfe_scores = pd.DataFrame({\n",
    "            'Features': all_features,\n",
    "            'RFE Support': rfe.support_,\n",
    "            'RFE Ranking': rfe.ranking_\n",
    "        })\n",
    "        rfe_scores = rfe_scores.sort_values(by='RFE Ranking')\n",
    "\n",
    "        # Selection Criteria\n",
    "        # To select the features to be used for the machine learning model, we'll consider the following criteria:\n",
    "\n",
    "        # 1. Features with high ANOVA F-statistic scores (for numerical features) or high chi-squared test scores (for categorical features).\n",
    "        # 2. Features with high feature importance scores (from Random Forest).\n",
    "        # 3. Features selected by the Recursive Feature Elimination (RFE) method.\n",
    "        # 4. We'll select the features that satisfy at least two of the above criteria. After selecting the features, we'll also remove features that are highly correlated with each other to avoid multicollinearity.\n",
    "\n",
    "        # Let's start by selecting the features based on the above criteria.\n",
    "        # Initialize a dictionary to keep track of how many criteria each feature satisfies\n",
    "        feature_criteria_counts = {}\n",
    "\n",
    "        # Define a function to add features that meet a criterion to the dictionary\n",
    "        def add_features(features, criterion_name):\n",
    "            for feature in features:\n",
    "                if feature not in feature_criteria_counts:\n",
    "                    feature_criteria_counts[feature] = 1\n",
    "                else:\n",
    "                    feature_criteria_counts[feature] += 1\n",
    "\n",
    "        # Select features with high ANOVA F-statistic scores\n",
    "        anova_selected = set(anova_scores[anova_scores['ANOVA F-statistic'] > 10]['Numerical Features'])\n",
    "        add_features(anova_selected, 'ANOVA')\n",
    "\n",
    "        # Select features with high chi-squared test scores\n",
    "        chi2_selected = set(chi2_scores[chi2_scores['Chi-Squared Test'] > 10]['Categorical Features'])\n",
    "        add_features(chi2_selected, 'Chi-Squared')\n",
    "\n",
    "        # Select features with high feature importance scores\n",
    "        importance_selected = set(feature_importance_scores[feature_importance_scores['Feature Importances'] > 0.03]['Features'])\n",
    "        add_features(importance_selected, 'Feature Importance')\n",
    "\n",
    "        # Select features selected by RFE\n",
    "        rfe_selected = set(rfe_scores[rfe_scores['RFE Support']]['Features'])\n",
    "        add_features(rfe_selected, 'RFE')\n",
    "\n",
    "        # Identify features that satisfy at least two criteria\n",
    "        selected_features = [feature for feature, count in feature_criteria_counts.items() if count >= 2]\n",
    "\n",
    "        # Create a DataFrame with only the selected features\n",
    "        selected_df = pd.DataFrame(X, columns=all_features)[selected_features]\n",
    "\n",
    "        # Remove features that are highly correlated with each other\n",
    "        correlation_matrix = selected_df.corr()\n",
    "        upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape, dtype=bool), k=1))\n",
    "        highly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.8)]\n",
    "\n",
    "        # Remove the highly correlated features\n",
    "        for feature in highly_correlated_features:\n",
    "            if feature in selected_features:\n",
    "                selected_features.remove(feature)\n",
    "        #X_selected=df[selected_features]\n",
    "        # Create a DataFrame with only the selected features\n",
    "        X_selected = pd.DataFrame(X, columns=all_features)[selected_features]\n",
    "        X_selected.to_csv(os.path.join(self.config.root_dir, \"X_selected.csv\"),index = False)\n",
    "        \n",
    "        joblib.dump(preprocessor,(os.path.join(self.config.root_dir, \"preprocessor.pkl\")))\n",
    "        joblib.dump(selected_features,(os.path.join(self.config.root_dir, \"selected_features.pkl\")))\n",
    "        \n",
    "\n",
    "\n",
    "    def train_test_spliting(self):\n",
    "        data = pd.read_csv(self.config.data_path)\n",
    "        X_selected = pd.read_csv(os.path.join(self.config.root_dir, \"X_selected.csv\"))\n",
    "        y = pd.read_csv(os.path.join(self.config.root_dir, \"y.csv\"))\n",
    "        # Split the data into training and test sets. (0.75, 0.25) split.\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "        X_train.to_csv(os.path.join(self.config.root_dir, \"X_train.csv\"),index = False)\n",
    "        X_test.to_csv(os.path.join(self.config.root_dir, \"X_test.csv\"),index = False)\n",
    "        y_train.to_csv(os.path.join(self.config.root_dir, \"y_train.csv\"),index = False)\n",
    "        y_test.to_csv(os.path.join(self.config.root_dir, \"y_test.csv\"),index = False)\n",
    "\n",
    "        logger.info(\"Splited data into training and test sets\")\n",
    "        logger.info(X_train.shape)\n",
    "        logger.info(y_train.shape)\n",
    "\n",
    "        logger.info(X_test.shape)\n",
    "        logger.info(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-22 21:13:57,038: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2023-09-22 21:13:57,051: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-09-22 21:13:57,077: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2023-09-22 21:13:57,083: INFO: common: created directory at: artifacts]\n",
      "[2023-09-22 21:13:57,086: INFO: common: created directory at: artifacts/data_transformation]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-22 21:14:40,599: INFO: 452690105: Splited data into training and test sets]\n",
      "[2023-09-22 21:14:40,601: INFO: 452690105: (1176, 11)]\n",
      "[2023-09-22 21:14:40,604: INFO: 452690105: (1176, 1)]\n",
      "[2023-09-22 21:14:40,606: INFO: 452690105: (294, 11)]\n",
      "[2023-09-22 21:14:40,609: INFO: 452690105: (294, 1)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.data_preprocessing()\n",
    "    data_transformation.train_test_spliting()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
